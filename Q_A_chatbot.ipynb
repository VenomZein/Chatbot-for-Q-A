{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 5793551,
          "sourceType": "datasetVersion",
          "datasetId": 3327967
        },
        {
          "sourceId": 6369513,
          "sourceType": "datasetVersion",
          "datasetId": 3669776
        }
      ],
      "dockerImageVersionId": 30498,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "Q/A chatbot with LLMs + Harry Potter",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'faiss-hp-sentence-transformers:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F3669776%2F6369513%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240321%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240321T141322Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D48009a2d7e202ceedfb03f12b1314b368b80d9902452a2847fb3cdc736a645215740b9a023f563d5de0cd0283763dd88ec49a7ff2c514ccaa07aff9bbfa2bbd3be399381b87d824445d50bc61c8d166347d3dd4f6c9cbf1d925f29c1774a2d75c608c53123e2e64211924fbb603bf8c121c0d91012dfb9d7d4d685ba44db9b68119a8ecb479497f060577c131b5981e2c2504794e528f3fbe7b71f07700f0cfee83268d8c166a9529ba3bae615ce343beb87d279861c0741bc4786dc506324d8606f67d3f46c25fe8f54b8ab65836bb5fb342babb79cdc9fd53328185566bf27af021181311f188b8e909b1ddaa682c7fe1062f84fd392b42fa379677c55c6fc'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "4YCcaF5jXG5q"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "! nvidia-smi -L"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-03T04:49:04.98247Z",
          "iopub.execute_input": "2024-03-03T04:49:04.98292Z",
          "iopub.status.idle": "2024-03-03T04:49:05.985586Z",
          "shell.execute_reply.started": "2024-03-03T04:49:04.982886Z",
          "shell.execute_reply": "2024-03-03T04:49:05.984516Z"
        },
        "trusted": true,
        "id": "tCOmXt-wXG5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installs"
      ],
      "metadata": {
        "id": "IV2WrYedXG5y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "from IPython.display import clear_output\n",
        "\n",
        "! pip install sentence_transformers==2.2.2\n",
        "\n",
        "! pip install -qq -U langchain\n",
        "! pip install -qq -U tiktoken\n",
        "! pip install -qq -U pypdf\n",
        "! pip install -qq -U faiss-gpu\n",
        "! pip install -qq -U InstructorEmbedding\n",
        "\n",
        "! pip install -qq -U transformers\n",
        "! pip install -qq -U accelerate\n",
        "! pip install -qq -U bitsandbytes\n",
        "\n",
        "clear_output()"
      ],
      "metadata": {
        "_kg_hide-output": false,
        "execution": {
          "iopub.status.busy": "2024-03-03T04:49:05.988165Z",
          "iopub.execute_input": "2024-03-03T04:49:05.98858Z",
          "iopub.status.idle": "2024-03-03T04:50:50.589987Z",
          "shell.execute_reply.started": "2024-03-03T04:49:05.988537Z",
          "shell.execute_reply": "2024-03-03T04:50:50.588886Z"
        },
        "trusted": true,
        "id": "oP_cYOW0XG5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "enRJ4YKqXG5z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import textwrap\n",
        "import time\n",
        "\n",
        "import langchain\n",
        "\n",
        "### loaders\n",
        "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
        "\n",
        "### splits\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "### prompts\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "\n",
        "### vector stores\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "### models\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
        "\n",
        "### retrievers\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig,\n",
        "    pipeline\n",
        ")\n",
        "\n",
        "clear_output()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-03T04:50:50.591532Z",
          "iopub.execute_input": "2024-03-03T04:50:50.591912Z",
          "iopub.status.idle": "2024-03-03T04:51:03.032318Z",
          "shell.execute_reply.started": "2024-03-03T04:50:50.591881Z",
          "shell.execute_reply": "2024-03-03T04:51:03.03131Z"
        },
        "trusted": true,
        "id": "KLbyRpK3XG5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('langchain:', langchain.__version__)\n",
        "print('torch:', torch.__version__)\n",
        "print('transformers:', transformers.__version__)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-03T04:51:03.033381Z",
          "iopub.execute_input": "2024-03-03T04:51:03.033987Z",
          "iopub.status.idle": "2024-03-03T04:51:03.039577Z",
          "shell.execute_reply.started": "2024-03-03T04:51:03.033959Z",
          "shell.execute_reply": "2024-03-03T04:51:03.038619Z"
        },
        "trusted": true,
        "id": "hG4kDI7AXG50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted(glob.glob('/kaggle/input/harry-potter-books-in-pdf-1-7/HP books/*'))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-03T04:51:03.041973Z",
          "iopub.execute_input": "2024-03-03T04:51:03.042258Z",
          "iopub.status.idle": "2024-03-03T04:51:03.063129Z",
          "shell.execute_reply.started": "2024-03-03T04:51:03.042232Z",
          "shell.execute_reply": "2024-03-03T04:51:03.062135Z"
        },
        "trusted": true,
        "id": "Gr9WeNoXXG51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CFG\n",
        "\n",
        "- CFG class enables easy and organized experimentation"
      ],
      "metadata": {
        "id": "vVkOUcIFXG52"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CFG:\n",
        "    # LLMs\n",
        "    model_name = 'llama2-13b-chat' # wizardlm, llama2-7b-chat, llama2-13b-chat, mistral-7B\n",
        "    temperature = 0\n",
        "    top_p = 0.95\n",
        "    repetition_penalty = 1.15\n",
        "\n",
        "    # splitting\n",
        "    split_chunk_size = 800\n",
        "    split_overlap = 0\n",
        "\n",
        "    # embeddings\n",
        "    embeddings_model_repo = 'sentence-transformers/all-MiniLM-L6-v2'\n",
        "\n",
        "    # similar passages\n",
        "    k = 6\n",
        "\n",
        "    # paths\n",
        "    PDFs_path = '/kaggle/input/harry-potter-books-in-pdf-1-7/HP books/'\n",
        "    Embeddings_path =  '/kaggle/input/faiss-hp-sentence-transformers'\n",
        "    Output_folder = './harry-potter-vectordb'"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-03T04:51:03.064422Z",
          "iopub.execute_input": "2024-03-03T04:51:03.064727Z",
          "iopub.status.idle": "2024-03-03T04:51:03.069946Z",
          "shell.execute_reply.started": "2024-03-03T04:51:03.064702Z",
          "shell.execute_reply": "2024-03-03T04:51:03.069047Z"
        },
        "trusted": true,
        "id": "bTmQvyqGXG52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define model"
      ],
      "metadata": {
        "id": "yJmTK49mXG52"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model(model = CFG.model_name):\n",
        "\n",
        "    print('\\nDownloading model: ', model, '\\n\\n')\n",
        "\n",
        "    if model == 'wizardlm':\n",
        "        model_repo = 'TheBloke/wizardLM-7B-HF'\n",
        "\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_repo)\n",
        "\n",
        "        bnb_config = BitsAndBytesConfig(\n",
        "            load_in_4bit = True,\n",
        "            bnb_4bit_quant_type = \"nf4\",\n",
        "            bnb_4bit_compute_dtype = torch.float16,\n",
        "            bnb_4bit_use_double_quant = True,\n",
        "        )\n",
        "\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_repo,\n",
        "            quantization_config = bnb_config,\n",
        "            device_map = 'auto',\n",
        "            low_cpu_mem_usage = True\n",
        "        )\n",
        "\n",
        "        max_len = 1024\n",
        "\n",
        "    elif model == 'llama2-7b-chat':\n",
        "        model_repo = 'daryl149/llama-2-7b-chat-hf'\n",
        "\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_repo, use_fast=True)\n",
        "\n",
        "        bnb_config = BitsAndBytesConfig(\n",
        "            load_in_4bit = True,\n",
        "            bnb_4bit_quant_type = \"nf4\",\n",
        "            bnb_4bit_compute_dtype = torch.float16,\n",
        "            bnb_4bit_use_double_quant = True,\n",
        "        )\n",
        "\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_repo,\n",
        "            quantization_config = bnb_config,\n",
        "            device_map = 'auto',\n",
        "            low_cpu_mem_usage = True,\n",
        "            trust_remote_code = True\n",
        "        )\n",
        "\n",
        "        max_len = 2048\n",
        "\n",
        "    elif model == 'llama2-13b-chat':\n",
        "        model_repo = 'daryl149/llama-2-13b-chat-hf'\n",
        "\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_repo, use_fast=True)\n",
        "\n",
        "        bnb_config = BitsAndBytesConfig(\n",
        "            load_in_4bit = True,\n",
        "            bnb_4bit_quant_type = \"nf4\",\n",
        "            bnb_4bit_compute_dtype = torch.float16,\n",
        "            bnb_4bit_use_double_quant = True,\n",
        "        )\n",
        "\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_repo,\n",
        "            quantization_config = bnb_config,\n",
        "            device_map = 'auto',\n",
        "            low_cpu_mem_usage = True,\n",
        "            trust_remote_code = True\n",
        "        )\n",
        "\n",
        "        max_len = 2048 # 8192\n",
        "\n",
        "    elif model == 'mistral-7B':\n",
        "        model_repo = 'mistralai/Mistral-7B-v0.1'\n",
        "\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_repo)\n",
        "\n",
        "        bnb_config = BitsAndBytesConfig(\n",
        "            load_in_4bit = True,\n",
        "            bnb_4bit_quant_type = \"nf4\",\n",
        "            bnb_4bit_compute_dtype = torch.float16,\n",
        "            bnb_4bit_use_double_quant = True,\n",
        "        )\n",
        "\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_repo,\n",
        "            quantization_config = bnb_config,\n",
        "            device_map = 'auto',\n",
        "            low_cpu_mem_usage = True,\n",
        "        )\n",
        "\n",
        "        max_len = 1024\n",
        "\n",
        "    else:\n",
        "        print(\"Not implemented model (tokenizer and backbone)\")\n",
        "\n",
        "    return tokenizer, model, max_len"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-03T04:51:03.071129Z",
          "iopub.execute_input": "2024-03-03T04:51:03.071375Z",
          "iopub.status.idle": "2024-03-03T04:51:03.084886Z",
          "shell.execute_reply.started": "2024-03-03T04:51:03.071353Z",
          "shell.execute_reply": "2024-03-03T04:51:03.084045Z"
        },
        "trusted": true,
        "id": "FgWq-LvoXG53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "tokenizer, model, max_len = get_model(model = CFG.model_name)\n",
        "\n",
        "clear_output()"
      ],
      "metadata": {
        "_kg_hide-output": false,
        "execution": {
          "iopub.status.busy": "2024-03-03T04:51:03.087567Z",
          "iopub.execute_input": "2024-03-03T04:51:03.08786Z",
          "iopub.status.idle": "2024-03-03T04:53:23.802829Z",
          "shell.execute_reply.started": "2024-03-03T04:51:03.087835Z",
          "shell.execute_reply": "2024-03-03T04:53:23.801631Z"
        },
        "trusted": true,
        "id": "7MNthSxcXG53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()"
      ],
      "metadata": {
        "_kg_hide-output": true,
        "execution": {
          "iopub.status.busy": "2024-03-03T04:53:23.80415Z",
          "iopub.execute_input": "2024-03-03T04:53:23.8045Z",
          "iopub.status.idle": "2024-03-03T04:53:23.819144Z",
          "shell.execute_reply.started": "2024-03-03T04:53:23.804468Z",
          "shell.execute_reply": "2024-03-03T04:53:23.818054Z"
        },
        "trusted": true,
        "id": "bOUo-PpqXG53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### check how Accelerate split the model across the available devices (GPUs)\n",
        "model.hf_device_map"
      ],
      "metadata": {
        "_kg_hide-output": true,
        "execution": {
          "iopub.status.busy": "2024-03-03T04:53:23.820482Z",
          "iopub.execute_input": "2024-03-03T04:53:23.820832Z",
          "iopub.status.idle": "2024-03-03T04:53:23.834429Z",
          "shell.execute_reply.started": "2024-03-03T04:53:23.820802Z",
          "shell.execute_reply": "2024-03-03T04:53:23.833281Z"
        },
        "trusted": true,
        "id": "WJE9gx_LXG54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ¤— pipeline\n",
        "\n",
        "- Hugging Face pipeline"
      ],
      "metadata": {
        "id": "bLGUmalZXG54"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### hugging face pipeline\n",
        "pipe = pipeline(\n",
        "    task = \"text-generation\",\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    pad_token_id = tokenizer.eos_token_id,\n",
        "#     do_sample = True,\n",
        "    max_length = max_len,\n",
        "    temperature = CFG.temperature,\n",
        "    top_p = CFG.top_p,\n",
        "    repetition_penalty = CFG.repetition_penalty\n",
        ")\n",
        "\n",
        "### langchain pipeline\n",
        "llm = HuggingFacePipeline(pipeline = pipe)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-03T04:53:23.835898Z",
          "iopub.execute_input": "2024-03-03T04:53:23.836219Z",
          "iopub.status.idle": "2024-03-03T04:53:23.845654Z",
          "shell.execute_reply.started": "2024-03-03T04:53:23.836191Z",
          "shell.execute_reply": "2024-03-03T04:53:23.844799Z"
        },
        "trusted": true,
        "id": "ZoznfHntXG54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-03T04:53:23.846908Z",
          "iopub.execute_input": "2024-03-03T04:53:23.847216Z",
          "iopub.status.idle": "2024-03-03T04:53:23.857226Z",
          "shell.execute_reply.started": "2024-03-03T04:53:23.847189Z",
          "shell.execute_reply": "2024-03-03T04:53:23.856223Z"
        },
        "trusted": true,
        "id": "E4jVjOyEXG54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "### testing model, not using the harry potter books yet\n",
        "### answer is not necessarily related to harry potter\n",
        "query = \"Give me 5 examples of cool potions and explain what they do\"\n",
        "llm.invoke(query)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-03T04:53:23.858245Z",
          "iopub.execute_input": "2024-03-03T04:53:23.858505Z",
          "iopub.status.idle": "2024-03-03T04:54:05.862672Z",
          "shell.execute_reply.started": "2024-03-03T04:53:23.858473Z",
          "shell.execute_reply": "2024-03-03T04:54:05.861701Z"
        },
        "trusted": true,
        "id": "YpuDCfcUXG54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ¦œðŸ”— Langchain\n",
        "\n",
        "- Multiple document retriever with LangChain"
      ],
      "metadata": {
        "id": "94j5kq6lXG54"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CFG.model_name"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-03T04:54:05.867225Z",
          "iopub.execute_input": "2024-03-03T04:54:05.867536Z",
          "iopub.status.idle": "2024-03-03T04:54:05.873505Z",
          "shell.execute_reply.started": "2024-03-03T04:54:05.867507Z",
          "shell.execute_reply": "2024-03-03T04:54:05.872533Z"
        },
        "trusted": true,
        "id": "DSaFxbhwXG55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loader\n",
        "\n",
        "- [Directory loader](https://python.langchain.com/docs/modules/data_connection/document_loaders/file_directory) for multiple files\n",
        "- This step is not necessary if you are just loading the vector database\n",
        "- This step is necessary if you are creating embeddings. In this case you need to:\n",
        "    - load de PDF files\n",
        "    - split into chunks\n",
        "    - create embeddings\n",
        "    - save the embeddings in a vector store\n",
        "    - After that you can just load the saved embeddings to do similarity search with the user query, and then use the LLM to answer the question\n",
        "    \n",
        "You can comment out this section if you use the embeddings I already created."
      ],
      "metadata": {
        "id": "j4sO_WNcXG55"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "loader = DirectoryLoader(\n",
        "    CFG.PDFs_path,\n",
        "    glob=\"./*.pdf\",\n",
        "    loader_cls=PyPDFLoader,\n",
        "    show_progress=True,\n",
        "    use_multithreading=True\n",
        ")\n",
        "\n",
        "documents = loader.load()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-03T04:54:05.874703Z",
          "iopub.execute_input": "2024-03-03T04:54:05.875112Z",
          "iopub.status.idle": "2024-03-03T04:56:26.454696Z",
          "shell.execute_reply.started": "2024-03-03T04:54:05.875077Z",
          "shell.execute_reply": "2024-03-03T04:56:26.453617Z"
        },
        "trusted": true,
        "id": "hUs5-brbXG55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'We have {len(documents)} pages in total')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-03T04:56:26.456408Z",
          "iopub.execute_input": "2024-03-03T04:56:26.45721Z",
          "iopub.status.idle": "2024-03-03T04:56:26.462709Z",
          "shell.execute_reply.started": "2024-03-03T04:56:26.457168Z",
          "shell.execute_reply": "2024-03-03T04:56:26.461769Z"
        },
        "trusted": true,
        "id": "m1JomlPxXG55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents[8].page_content"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-03T04:56:26.464164Z",
          "iopub.execute_input": "2024-03-03T04:56:26.464473Z",
          "iopub.status.idle": "2024-03-03T04:56:26.476236Z",
          "shell.execute_reply.started": "2024-03-03T04:56:26.464446Z",
          "shell.execute_reply": "2024-03-03T04:56:26.475207Z"
        },
        "trusted": true,
        "id": "V58Ka7miXG55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Splitter\n",
        "\n",
        "- Splitting the text into chunks so its passages are easily searchable for similarity\n",
        "- This step is also only necessary if you are creating the embeddings\n",
        "- [RecursiveCharacterTextSplitter](https://python.langchain.com/en/latest/reference/modules/document_loaders.html?highlight=RecursiveCharacterTextSplitter#langchain.document_loaders.MWDumpLoader)"
      ],
      "metadata": {
        "id": "ZzHyxsdnXG55"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = CFG.split_chunk_size,\n",
        "    chunk_overlap = CFG.split_overlap\n",
        ")\n",
        "\n",
        "texts = text_splitter.split_documents(documents)\n",
        "\n",
        "print(f'We have created {len(texts)} chunks from {len(documents)} pages')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-03T04:56:26.47748Z",
          "iopub.execute_input": "2024-03-03T04:56:26.477817Z",
          "iopub.status.idle": "2024-03-03T04:56:27.592563Z",
          "shell.execute_reply.started": "2024-03-03T04:56:26.477778Z",
          "shell.execute_reply": "2024-03-03T04:56:27.591429Z"
        },
        "trusted": true,
        "id": "85dYa5BOXG55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Embeddings\n",
        "\n",
        "\n",
        "- Embedd and store the texts in a Vector database (FAISS)\n",
        "- [LangChain Vector Stores docs](https://python.langchain.com/docs/modules/data_connection/vectorstores/)\n",
        "- [FAISS - langchain](https://python.langchain.com/docs/integrations/vectorstores/faiss)\n",
        "- [Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks - paper Aug/2019](https://arxiv.org/pdf/1908.10084.pdf)\n",
        "- [This is a nice 4 minutes video about vector stores](https://www.youtube.com/watch?v=dN0lsF2cvm4)\n",
        "\n",
        "___\n",
        "\n",
        "- If you use Chroma vector store it will take ~35 min to create embeddings\n",
        "- If you use FAISS vector store on GPU it will take just ~3 min\n",
        "\n",
        "___\n",
        "\n",
        "We need to create the embeddings only once, and then we can just load the vector store and query the database using similarity search.\n",
        "\n",
        "Loading the embeddings takes only a few seconds.\n",
        "\n",
        "I uploaded the embeddings to a Kaggle Dataset so we just load it from [here](https://www.kaggle.com/datasets/hinepo/faiss-hp-sentence-transformers)."
      ],
      "metadata": {
        "id": "QIw2u1kNXG56"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "### we create the embeddings only if they do not exist yet\n",
        "if not os.path.exists(CFG.Embeddings_path + '/index.faiss'):\n",
        "\n",
        "    ### download embeddings model\n",
        "    embeddings = HuggingFaceInstructEmbeddings(\n",
        "        model_name = CFG.embeddings_model_repo,\n",
        "        model_kwargs = {\"device\": \"cuda\"}\n",
        "    )\n",
        "\n",
        "    ### create embeddings and DB\n",
        "    vectordb = FAISS.from_documents(\n",
        "        documents = texts,\n",
        "        embedding = embeddings\n",
        "    )\n",
        "\n",
        "    ### persist vector database\n",
        "    vectordb.save_local(f\"{CFG.Output_folder}/faiss_index_hp\") # save in output folder\n",
        "#     vectordb.save_local(f\"{CFG.Embeddings_path}/faiss_index_hp\") # save in input folder"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-03T04:56:27.594158Z",
          "iopub.execute_input": "2024-03-03T04:56:27.5946Z",
          "iopub.status.idle": "2024-03-03T04:56:27.603829Z",
          "shell.execute_reply.started": "2024-03-03T04:56:27.594561Z",
          "shell.execute_reply": "2024-03-03T04:56:27.602693Z"
        },
        "trusted": true,
        "id": "JbIphkR6XG56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If creating embeddings, remember that on Kaggle we can not write data to the input folder.\n",
        "\n",
        "So just write (save) the embeddings to the output folder and then load them from there."
      ],
      "metadata": {
        "id": "YAkIbZ-6XG56"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load vector database\n",
        "\n",
        "- After saving the vector database, we just load it from the Kaggle Dataset I mentioned\n",
        "- Obviously, the embeddings function to load the embeddings must be the same as the one used to create the embeddings"
      ],
      "metadata": {
        "id": "DKMwn8FbXG56"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "### download embeddings model\n",
        "embeddings = HuggingFaceInstructEmbeddings(\n",
        "    model_name = CFG.embeddings_model_repo,\n",
        "    model_kwargs = {\"device\": \"cuda\"}\n",
        ")\n",
        "\n",
        "### load vector DB embeddings\n",
        "vectordb = FAISS.load_local(\n",
        "    CFG.Embeddings_path, # from input folder\n",
        "#     CFG.Output_folder + '/faiss_index_hp', # from output folder\n",
        "    embeddings\n",
        ")\n",
        "\n",
        "clear_output()"
      ],
      "metadata": {
        "_kg_hide-output": false,
        "execution": {
          "iopub.status.busy": "2024-03-03T04:56:27.605407Z",
          "iopub.execute_input": "2024-03-03T04:56:27.605867Z",
          "iopub.status.idle": "2024-03-03T04:56:29.883531Z",
          "shell.execute_reply.started": "2024-03-03T04:56:27.605836Z",
          "shell.execute_reply": "2024-03-03T04:56:29.882458Z"
        },
        "trusted": true,
        "id": "yMuIyB1RXG56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### test if vector DB was loaded correctly\n",
        "vectordb.similarity_search('magic creatures')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-03T04:56:29.884796Z",
          "iopub.execute_input": "2024-03-03T04:56:29.885122Z",
          "iopub.status.idle": "2024-03-03T04:56:30.402706Z",
          "shell.execute_reply.started": "2024-03-03T04:56:29.885092Z",
          "shell.execute_reply": "2024-03-03T04:56:30.401809Z"
        },
        "trusted": true,
        "id": "IISHnbmSXG57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt Template\n",
        "\n",
        "- Custom prompt"
      ],
      "metadata": {
        "id": "dM0hUy1OXG57"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = \"\"\"\n",
        "Don't try to make up an answer, if you don't know just say that you don't know.\n",
        "Answer in the same language the question was asked.\n",
        "Use only the following pieces of context to answer the question at the end.\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "Answer:\"\"\"\n",
        "\n",
        "\n",
        "PROMPT = PromptTemplate(\n",
        "    template = prompt_template,\n",
        "    input_variables = [\"context\", \"question\"]\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-03T04:56:30.403842Z",
          "iopub.execute_input": "2024-03-03T04:56:30.404115Z",
          "iopub.status.idle": "2024-03-03T04:56:30.409596Z",
          "shell.execute_reply.started": "2024-03-03T04:56:30.404091Z",
          "shell.execute_reply": "2024-03-03T04:56:30.408569Z"
        },
        "trusted": true,
        "id": "TOyC3CMeXG57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# llm_chain = LLMChain(prompt=PROMPT, llm=llm)\n",
        "# llm_chain"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-03T04:56:30.410886Z",
          "iopub.execute_input": "2024-03-03T04:56:30.411175Z",
          "iopub.status.idle": "2024-03-03T04:56:30.421165Z",
          "shell.execute_reply.started": "2024-03-03T04:56:30.41115Z",
          "shell.execute_reply": "2024-03-03T04:56:30.420262Z"
        },
        "trusted": true,
        "id": "W0oSA6XiXG57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retriever chain\n",
        "\n",
        "- Retriever to retrieve relevant passages\n",
        "- Chain to answer questions\n",
        "- [RetrievalQA: Chain for question-answering](https://python.langchain.com/docs/modules/data_connection/retrievers/)"
      ],
      "metadata": {
        "id": "q_hN54hqXG57"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vectordb.as_retriever(search_kwargs = {\"k\": CFG.k, \"search_type\" : \"similarity\"})\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm = llm,\n",
        "    chain_type = \"stuff\", # map_reduce, map_rerank, stuff, refine\n",
        "    retriever = retriever,\n",
        "    chain_type_kwargs = {\"prompt\": PROMPT},\n",
        "    return_source_documents = True,\n",
        "    verbose = False\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-03T04:56:30.42236Z",
          "iopub.execute_input": "2024-03-03T04:56:30.422661Z",
          "iopub.status.idle": "2024-03-03T04:56:30.43323Z",
          "shell.execute_reply.started": "2024-03-03T04:56:30.422625Z",
          "shell.execute_reply": "2024-03-03T04:56:30.432288Z"
        },
        "trusted": true,
        "id": "efjDVjQUXG6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### testing MMR search\n",
        "question = \"Which are Hagrid's favorite animals?\"\n",
        "vectordb.max_marginal_relevance_search(question, k = CFG.k)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-03T04:56:30.434349Z",
          "iopub.execute_input": "2024-03-03T04:56:30.434654Z",
          "iopub.status.idle": "2024-03-03T04:56:30.495228Z",
          "shell.execute_reply.started": "2024-03-03T04:56:30.434628Z",
          "shell.execute_reply": "2024-03-03T04:56:30.494286Z"
        },
        "trusted": true,
        "id": "naiFYfc1XG6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### testing similarity search\n",
        "question = \"Which are Hagrid's favorite animals?\"\n",
        "vectordb.similarity_search(question, k = CFG.k)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-03T04:56:30.496296Z",
          "iopub.execute_input": "2024-03-03T04:56:30.496552Z",
          "iopub.status.idle": "2024-03-03T04:56:30.522625Z",
          "shell.execute_reply.started": "2024-03-03T04:56:30.496528Z",
          "shell.execute_reply": "2024-03-03T04:56:30.52166Z"
        },
        "trusted": true,
        "id": "hyPzSratXG6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Post-process outputs\n",
        "\n",
        "- Format llm response\n",
        "- Cite sources (PDFs)\n",
        "- Change `width` parameter to format the output"
      ],
      "metadata": {
        "id": "Z3ue8SBHXG6B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def wrap_text_preserve_newlines(text, width=700):\n",
        "    # Split the input text into lines based on newline characters\n",
        "    lines = text.split('\\n')\n",
        "\n",
        "    # Wrap each line individually\n",
        "    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
        "\n",
        "    # Join the wrapped lines back together using newline characters\n",
        "    wrapped_text = '\\n'.join(wrapped_lines)\n",
        "\n",
        "    return wrapped_text\n",
        "\n",
        "\n",
        "def process_llm_response(llm_response):\n",
        "    ans = wrap_text_preserve_newlines(llm_response['result'])\n",
        "\n",
        "    sources_used = ' \\n'.join(\n",
        "        [\n",
        "            source.metadata['source'].split('/')[-1][:-4]\n",
        "            + ' - page: '\n",
        "            + str(source.metadata['page'])\n",
        "            for source in llm_response['source_documents']\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    ans = ans + '\\n\\nSources: \\n' + sources_used\n",
        "    return ans"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-03T04:56:30.5241Z",
          "iopub.execute_input": "2024-03-03T04:56:30.524496Z",
          "iopub.status.idle": "2024-03-03T04:56:30.532112Z",
          "shell.execute_reply.started": "2024-03-03T04:56:30.524459Z",
          "shell.execute_reply": "2024-03-03T04:56:30.531045Z"
        },
        "trusted": true,
        "id": "4iiv_mrpXG6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def llm_ans(query):\n",
        "    start = time.time()\n",
        "\n",
        "    llm_response = qa_chain.invoke(query)\n",
        "    ans = process_llm_response(llm_response)\n",
        "\n",
        "    end = time.time()\n",
        "\n",
        "    time_elapsed = int(round(end - start, 0))\n",
        "    time_elapsed_str = f'\\n\\nTime elapsed: {time_elapsed} s'\n",
        "    return ans + time_elapsed_str"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-03T04:56:30.533262Z",
          "iopub.execute_input": "2024-03-03T04:56:30.533556Z",
          "iopub.status.idle": "2024-03-03T04:56:30.542074Z",
          "shell.execute_reply.started": "2024-03-03T04:56:30.533529Z",
          "shell.execute_reply": "2024-03-03T04:56:30.541111Z"
        },
        "trusted": true,
        "id": "MHXixFS5XG6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ask questions\n",
        "\n",
        "- Question Answering from multiple documents\n",
        "- Invoke QA Chain\n",
        "- Talk to your data"
      ],
      "metadata": {
        "id": "p4uPPjYaXG6B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CFG.model_name"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-03T04:56:30.543255Z",
          "iopub.execute_input": "2024-03-03T04:56:30.54353Z",
          "iopub.status.idle": "2024-03-03T04:56:30.552891Z",
          "shell.execute_reply.started": "2024-03-03T04:56:30.543505Z",
          "shell.execute_reply": "2024-03-03T04:56:30.551908Z"
        },
        "trusted": true,
        "id": "SPeTQdBHXG6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Which challenges does Harry face during the Triwizard Tournament?\"\n",
        "print(llm_ans(query))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-03T04:56:30.554201Z",
          "iopub.execute_input": "2024-03-03T04:56:30.55449Z",
          "iopub.status.idle": "2024-03-03T04:56:44.068201Z",
          "shell.execute_reply.started": "2024-03-03T04:56:30.554464Z",
          "shell.execute_reply": "2024-03-03T04:56:44.067025Z"
        },
        "trusted": true,
        "id": "Xd86Fz7PXG6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Is Malfoy an ally of Voldemort?\"\n",
        "print(llm_ans(query))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-03T04:56:44.069536Z",
          "iopub.execute_input": "2024-03-03T04:56:44.069867Z",
          "iopub.status.idle": "2024-03-03T04:56:48.40032Z",
          "shell.execute_reply.started": "2024-03-03T04:56:44.069838Z",
          "shell.execute_reply": "2024-03-03T04:56:48.399339Z"
        },
        "trusted": true,
        "id": "JH8x1AH1XG6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What are horcrux?\"\n",
        "print(llm_ans(query))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-03T04:56:48.401764Z",
          "iopub.execute_input": "2024-03-03T04:56:48.402112Z",
          "iopub.status.idle": "2024-03-03T04:56:57.075701Z"
        },
        "trusted": true,
        "id": "DpDwxvFwXG6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Give me 5 examples of cool potions and explain what they do\"\n",
        "print(llm_ans(query))"
      ],
      "metadata": {
        "trusted": true,
        "id": "qVpA8ICwXG6C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}